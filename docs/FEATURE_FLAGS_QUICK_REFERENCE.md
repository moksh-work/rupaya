# Feature Flags Quick Reference

## üöÄ Common Operations

### Enable a Feature (Gradual Rollout)

```bash
# Start at 1%
curl -X PUT http://localhost:3000/api/admin/deployment/feature-flags/feature.new-dashboard \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"enabled": true, "percentage": 1}'

# Next day: increase to 10%
curl -X PUT http://localhost:3000/api/admin/deployment/feature-flags/feature.new-dashboard \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -d '{"percentage": 10}'

# Full rollout: 100%
curl -X PUT http://localhost:3000/api/admin/deployment/feature-flags/feature.new-dashboard \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -d '{"percentage": 100}'
```

### Launch Canary Deployment

```bash
# Start canary (begins at 1%)
curl -X PUT http://localhost:3000/api/admin/deployment/feature-flags/canary.new-payment \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -d '{"enabled": true}'

# System auto-advances: 1% ‚Üí 10% ‚Üí 25% ‚Üí 50% ‚Üí 100%
# Each stage: wait configured duration
# Health monitored: auto-rollback if errors spike

# Manually advance to next stage
curl -X POST http://localhost:3000/api/admin/deployment/feature-flags/canary.new-payment/advance-canary \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -d '{"reason": "Metrics look good"}'

# Rollback if problems detected
curl -X POST http://localhost:3000/api/admin/deployment/feature-flags/canary.new-payment/rollback-canary \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -d '{"reason": "Error rate spike detected"}'
```

### Run A/B Test

```bash
# Create 50/50 experiment
curl -X PUT http://localhost:3000/api/admin/deployment/feature-flags/experiment.checkout \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -d '{
    "enabled": true,
    "type": "experiment",
    "variants": {
      "control": {"percentage": 50},
      "variant_a": {"percentage": 50}
    }
  }'

# After 1+ week, check winner
curl -X GET "http://localhost:3000/api/admin/deployment/experiments/experiment.checkout/statistical-significance" \
  -H "Authorization: Bearer $ADMIN_TOKEN"

# Response: winner with confidence % and p-value
# If p-value < 0.05 (95% confidence), deploy winner

# Deploy winning variant
curl -X PUT http://localhost:3000/api/admin/deployment/feature-flags/experiment.checkout \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -d '{
    "variants": {
      "variant_a": {"percentage": 100},
      "control": {"percentage": 0}
    }
  }'
```

### Monitor Health

```bash
# Current deployment status
curl http://localhost:3000/health

# Metrics for last hour
curl "http://localhost:3000/api/admin/deployment/metrics/range?startTime=2024-01-15T00:00:00Z&endTime=2024-01-15T23:59:59Z" \
  -H "Authorization: Bearer $ADMIN_TOKEN"

# Get all rollback decisions
curl "http://localhost:3000/api/admin/deployment/rollback-decisions?limit=10" \
  -H "Authorization: Bearer $ADMIN_TOKEN"
```

---

## üìã API Endpoints Cheat Sheet

### Flag Management

| Method | Endpoint | Purpose |
|--------|----------|---------|
| GET | `/api/admin/deployment/feature-flags` | List all flags |
| GET | `/api/admin/deployment/feature-flags/{key}` | Get specific flag |
| PUT | `/api/admin/deployment/feature-flags/{key}` | Update flag |
| POST | `/api/admin/deployment/feature-flags/{key}/advance-canary` | Next canary stage |
| POST | `/api/admin/deployment/feature-flags/{key}/rollback-canary` | Prev canary stage |

### Metrics & Health

| Method | Endpoint | Purpose |
|--------|----------|---------|
| GET | `/health` | Health with metrics |
| GET | `/api/admin/deployment/metrics/health` | Current health status |
| GET | `/api/admin/deployment/metrics/range` | Time-range metrics |
| GET | `/api/admin/deployment/metrics/flag-usage` | Flag eval metrics |

### Experiments

| Method | Endpoint | Purpose |
|--------|----------|---------|
| GET | `/api/admin/deployment/experiments/{key}/results` | Variant results |
| GET | `/api/admin/deployment/experiments/{key}/statistical-significance` | A/B test winner |

---

## üîë Environment Variables

```bash
# Feature Flags Storage
FEATURE_FLAGS_CACHE_TTL=300                 # Redis cache duration (sec)

# Metrics Collection
METRICS_AGGREGATION_INTERVAL=10000          # How often to aggregate (ms)
METRICS_WINDOW_SIZE=60                      # Analysis window (sec)

# Rollback Thresholds
ROLLBACK_ERROR_RATE_THRESHOLD=5             # Error % for rollback
ROLLBACK_RESPONSE_TIME_THRESHOLD=2000       # Response time ms for rollback

# Admin API
ADMIN_API_TOKEN=abc123...                   # Generated by setup script

# Database & Cache
DATABASE_URL=postgresql://...
REDIS_URL=redis://...
```

---

## üí° Common Scenarios

### Scenario 1: Launch New Feature Safely

```
Day 1:  Enable 1%  ‚Üí Monitor for 2 hours
Day 2:  Enable 10% ‚Üí Monitor for 24 hours
Day 3:  Enable 50% ‚Üí Monitor for 24 hours
Day 4:  Enable 100% ‚Üí Keep monitoring for 48 hours
Day 6:  Remove flag from code (if permanent)
```

### Scenario 2: Emergency Revert

```bash
# Option 1: Disable feature immediately
curl -X PUT http://localhost:3000/api/admin/deployment/feature-flags/feature.problem \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"enabled": false}'

# Option 2: Rollback canary to 0%
curl -X POST http://localhost:3000/api/admin/deployment/feature-flags/canary.problem/rollback-canary \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"reason": "Critical bug in production"}'
```

### Scenario 3: A/B Test Decision

```bash
# Check results
GET /api/admin/deployment/experiments/experiment.checkout/results

# Analyze significance
GET /api/admin/deployment/experiments/experiment.checkout/statistical-significance

# If winner = "variant_a" and p < 0.05:
# 1. Deploy variant_a to 100%
# 2. Deploy code with variant_a as default
# 3. Remove variant_b from code
# 4. Delete experiment flag

# Else: continue test or investigate
```

---

## üßµ Code Integration Example

```javascript
// Route with multiple features
router.post('/api/checkout', async (req, res) => {
  try {
    // Check feature flags
    const newCheckout = await req.evaluateFlag('experiment.checkout-redesign');
    const aiPricing = await req.evaluateFlag('feature.ai-pricing');

    // Route to variant
    const checkoutUI = newCheckout.variant === 'control'
      ? renderCheckoutV1()
      : renderCheckoutV2();

    // Apply feature
    let pricing = calculatePricing(req.body.items);
    if (aiPricing.enabled) {
      pricing = await getAIPricingSuggestion(pricing);
    }

    // Record which variant user got (auto-tagged)
    req.metricsTags = {
      checkout_variant: newCheckout.variant,
      aiPricingEnabled: aiPricing.enabled
    };

    // Respond
    res.json({
      checkoutUI,
      pricing,
      pricing_source: aiPricing.enabled ? 'ai' : 'rules'
    });

  } catch (error) {
    // Metrics auto-recorded as error
    res.status(500).json({ error: error.message });
  }
});
```

All metrics automatically collected:
- ‚úÖ Response time
- ‚úÖ Error status
- ‚úÖ Variant assignment
- ‚úÖ Canary stage (if applicable)
- ‚úÖ Custom tags

---

## üìä Interpreting Metrics

### Health Status Response

```json
{
  "status": "healthy",
  "errorRate": 0.5,           // % of requests that errored
  "p99ResponseTime": 1200,    // 99th percentile response (ms)
  "requestsPerSecond": 150,   // Throughput
  "topErrors": [              // Most common errors
    {
      "error": "POST /api/payment - 500",
      "count": 15
    }
  ]
}
```

**Healthy** = errorRate < 5% AND p99ResponseTime < 2000ms  
**Degraded** = errorRate > 5% OR p99ResponseTime > 2000ms  
**Critical** = Will trigger auto-rollback

### Experiment Results

```json
{
  "variantResults": {
    "control": {
      "count": 5000,          // Sample size
      "errors": 25,           // Error count
      "errorRate": 0.5,       // Error %
      "avgResponseTime": 145  // Avg response (ms)
    },
    "variant_a": {
      "count": 5000,
      "errors": 15,
      "errorRate": 0.3,
      "avgResponseTime": 138
    }
  }
}
```

### Significance Test `Result

```json
{
  "winner": "variant_a",      // Better performing
  "isSignificant": true,      // p-value < 0.05
  "confidence": "95.43%",     // 1 - p-value
  "pValue": "0.0234"          // Statistical p-value
}
```

**When to deploy**:
- ‚úÖ `isSignificant: true`
- ‚úÖ `pValue < 0.05`
- ‚úÖ `confidence > 95%`

---

## üö® Alerts & Emergency Actions

### If Error Rate Spikes

```bash
# 1. Check what's happening
curl http://localhost:3000/api/admin/deployment/metrics/health

# 2. Find problematic endpoint
jq '.data.topErrors' <response>

# 3. Disable recent feature
curl -X PUT http://localhost:3000/api/admin/deployment/feature-flags/feature.recent \
  -d '{"enabled": false}'

# 4. Rollback canary if in progress
curl -X POST http://localhost:3000/api/admin/deployment/feature-flags/canary.recent/rollback-canary \
  -d '{"reason": "Error spike detected"}'

# 5. Monitor recovery
watch 'curl http://localhost:3000/health | jq .deployment'
```

### If Canary Stuck

```bash
# Check canary status
curl /api/admin/deployment/feature-flags/canary.something

# If stuck for too long:
# Option A: Advance manually
curl -X POST /api/admin/deployment/feature-flags/canary.something/advance-canary \
  -d '{"reason": "Force advance after manual verification"}'

# Option B: Rollback
curl -X POST /api/admin/deployment/feature-flags/canary.something/rollback-canary \
  -d '{"reason": "Metrics unclear, rolling back to previous stage"}'
```

---

## üì± Using GitHub UI

Go to: **Actions** ‚Üí **"Manage Feature Flags & Deployment"** ‚Üí **Run workflow**

Select:
1. **Action**: enable_flag, disable_flag, advance_canary, etc.
2. **Flag Key**: feature.xyz, canary.abc
3. **Percentage**: 0-100 (for rollout)
4. **Environment**: development, staging, production
5. **Reason**: Why you're making this change (audit)

Results appear in:
- Artifact: `feature-flag-management-results` folder
- Job summary: Formatted results and JSON response
- Logs: Curl command output and timestamps

---

## ‚úÖ Pre-Launch Checklist

```
Before enabling feature in production:
‚òê Tested in development (100%)
‚òê Tested in staging (10-50%)
‚òê All unit tests passing
‚òê Load tests passed
‚òê Database migrations completed
‚òê Rollback plan documented
‚òê Team knows what alerts to expect
‚òê Monitoring set up

During rollout in production:
‚òê Monitor error rate every 5 min
‚òê Monitor response times
‚òê Check support tickets
‚òê Database metrics normal
‚òê Memory/CPU healthy

After reaching 100%:
‚òê Monitor for 48 hours
‚òê No spike in errors/latency
‚òê Users report positive feedback
‚òê Remove experimental code (if applicable)
‚òê Follow up metrics analysis
```

---

**Last Updated**: January 15, 2024  
**Version**: 1.0  
**Status**: Production Ready ‚úÖ
